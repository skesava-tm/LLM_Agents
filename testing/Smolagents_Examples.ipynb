{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222f2d2b-1f19-4975-92c3-5e6a2a24e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, DuckDuckGoSearchTool, InferenceClientModel, TransformersModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de9c75-7eaf-413f-96a6-22f5e2b79557",
   "metadata": {},
   "source": [
    "# Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50accc59-f70b-42a7-87b0-e5d7cae7bd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffe3b8-b706-46d5-a0ff-349bfdf4a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1120e37d-efae-4939-b301-f3c4c335b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`max_new_tokens` not provided, using this default value for `max_new_tokens`: 4096\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"gpt2\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "model = TransformersModel(model_id=\"gpt2\")\n",
    "# model = TransformersModel(model_id=\"mistralai/Mistral-7B-v0.1\") # causing memory issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d81a90-177a-4743-a710-c304f6dea5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=InferenceClientModel())\n",
    "agent = CodeAgent(tools=[DuckDuckGoSearchTool()], model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc88eda-c3a3-49c6-ba02-e64a124e9910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb123132-a3a0-4c84-a8aa-63f829db5080",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569899a-d86c-44c4-8905-ee104f3059fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68eb64a-742a-4c45-bd0b-3aa860fd28c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edfc3bc-946c-4e5b-824c-0bf99f1120a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065d2b1-e090-41cf-8da5-263748fac177",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213dff1e-3368-48ef-9f93-c9dbde260bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokenized_text['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99dedce-bfc7-4007-ac44-b57384009122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d52db-9f8b-4f9d-a9fa-cf85fb6c696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(tokenized_text['input_ids']).view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c7a3e-6932-46a1-9b57-db878f00fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs=torch.tensor(tokenized_text['input_ids']).view(1,-1))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904539d3-e289-4351-8aba-b8ef39863052",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9053e2d-ed66-40a4-9bc6-a05515f47cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547923f0-38e6-40a5-b346-1468e3354edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96bc261b-e7c7-4a23-a5a3-bd8c17fa68f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">How many seconds would it take for a leopard at full speed to run through Pont des Arts?</span>                        <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ TransformersModel - gpt2 ──────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mHow many seconds would it take for a leopard at full speed to run through Pont des Arts?\u001b[0m                        \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m TransformersModel - gpt2 \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">For information about writing templates and setting the tokenizer.chat_template attribute, please see the </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">documentation at </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/main/en/chat_templating</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;31mCannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! \u001b[0m\n",
       "\u001b[1;31mFor information about writing templates and setting the tokenizer.chat_template attribute, please see the \u001b[0m\n",
       "\u001b[1;31mdocumentation at \u001b[0m\u001b[4;94mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.01 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.01 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AgentGenerationError",
     "evalue": "Error in generating model output:\nCannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\agents.py:1496\u001b[39m, in \u001b[36mCodeAgent._step_stream\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m   1495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1496\u001b[39m     chat_message: ChatMessage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1497\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<end_code>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mObservation:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCalling tools:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1499\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m     memory_step.model_output_message = chat_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\models.py:864\u001b[39m, in \u001b[36mTransformersModel.generate\u001b[39m\u001b[34m(self, messages, stop_sequences, response_format, tools_to_call_from, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTransformers does not support structured outputs, use VLLMModel for this.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m generation_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_completion_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools_to_call_from\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools_to_call_from\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m count_prompt_tokens = generation_kwargs[\u001b[33m\"\u001b[39m\u001b[33minputs\u001b[39m\u001b[33m\"\u001b[39m].shape[\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\models.py:830\u001b[39m, in \u001b[36mTransformersModel._prepare_completion_args\u001b[39m\u001b[34m(self, messages, stop_sequences, tools_to_call_from, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m max_new_tokens = (\n\u001b[32m    824\u001b[39m     kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmax_new_tokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    825\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   (...)\u001b[39m\u001b[32m    828\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1024\u001b[39m\n\u001b[32m    829\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m830\u001b[39m prompt_tensor = \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mget_tool_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtools_to_call_from\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtools_to_call_from\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtools_to_call_from\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    838\u001b[39m prompt_tensor = prompt_tensor.to(\u001b[38;5;28mself\u001b[39m.model.device)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1629\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m     tokenizer_kwargs = {}\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m chat_template = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re.search(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m{\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m%\u001b[39m\u001b[33m-?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*generation\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*-?\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m%\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m, chat_template):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1822\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.get_chat_template\u001b[39m\u001b[34m(self, chat_template, tools)\u001b[39m\n\u001b[32m   1821\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1823\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1824\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33margument was passed! For information about writing templates and setting the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1825\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1826\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1827\u001b[39m         )\n\u001b[32m   1829\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[31mValueError\u001b[39m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAgentGenerationError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHow many seconds would it take for a leopard at full speed to run through Pont des Arts?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\agents.py:399\u001b[39m, in \u001b[36mMultiStepAgent.run\u001b[39m\u001b[34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[39m\n\u001b[32m    396\u001b[39m run_start_time = time.time()\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Outputs are returned only at the end. We only look at the last step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m steps = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(steps[-\u001b[32m1\u001b[39m], FinalAnswerStep)\n\u001b[32m    401\u001b[39m output = steps[-\u001b[32m1\u001b[39m].output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\agents.py:478\u001b[39m, in \u001b[36mMultiStepAgent._run_stream\u001b[39m\u001b[34m(self, task, max_steps, images)\u001b[39m\n\u001b[32m    475\u001b[39m     final_answer = el\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[39;00m\n\u001b[32m    481\u001b[39m     action_step.error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\agents.py:473\u001b[39m, in \u001b[36mMultiStepAgent._run_stream\u001b[39m\u001b[34m(self, task, max_steps, images)\u001b[39m\n\u001b[32m    467\u001b[39m action_step = ActionStep(\n\u001b[32m    468\u001b[39m     step_number=\u001b[38;5;28mself\u001b[39m.step_number,\n\u001b[32m    469\u001b[39m     timing=Timing(start_time=action_step_start_time),\n\u001b[32m    470\u001b[39m     observations_images=images,\n\u001b[32m    471\u001b[39m )\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n\u001b[32m    475\u001b[39m     final_answer = el\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\agents.py:495\u001b[39m, in \u001b[36mMultiStepAgent._execute_step\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_execute_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, memory_step: ActionStep) -> Generator[ChatMessageStreamDelta | FinalOutput]:\n\u001b[32m    494\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.log_rule(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.step_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, level=LogLevel.INFO)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfinal_answer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatMessageStreamDelta\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\smolagents\\agents.py:1518\u001b[39m, in \u001b[36mCodeAgent._step_stream\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m   1516\u001b[39m     memory_step.model_output = output_text\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentGenerationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in generating model output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.logger) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1520\u001b[39m \u001b[38;5;66;03m### Parse output ###\u001b[39;00m\n\u001b[32m   1521\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAgentGenerationError\u001b[39m: Error in generating model output:\nCannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a48210-009c-4dca-9294-36b1bd0b9a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d4f01-ddf0-4587-93fd-109aa6714139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f54409e-8a61-4f6e-b608-973e99e2f79e",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c351e16-fc1d-4d15-997f-0c01efe07452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai # openai v1.0.0+\n",
    "client = openai.OpenAI(api_key=\"anything\",base_url=\"http://0.0.0.0:4000\") # set proxy to base_url\n",
    "# request sent to model set on litellm proxy, `litellm --model`\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"this is a test request, write a short poem\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c4793-b9b0-4d89-9431-f4735034fc63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "llm_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
