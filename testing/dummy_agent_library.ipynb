{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9abf449-23fb-4ccd-b2b8-8313d02f5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "import dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da06ba64-1dc1-4065-af5e-16c3b5a8435e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You need a token from https://hf.co/settings/tokens, ensure that you select 'read' as the token type. If you run this on Google Colab, you can set it up in the \"settings\" tab under \"secrets\". Make sure to call it \"HF_TOKEN\"\n",
    "env_loaded = dotenv.load_dotenv(\"../.env\")\n",
    "\n",
    "if env_loaded:\n",
    "    pass\n",
    "else:\n",
    "    raise FileNotFoundError(\"Environment file with HF TOKEN not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8a0d35-7933-42ce-b4d9-27f1a3c1e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(model=\"gpt2\", provider=\"hf-inference\", token=os.environ[\"HF_TOKEN\"])\n",
    "# if the outputs for next cells are wrong, the free model may be overloaded. You can also use this public endpoint that contains Llama-3.2-3B-Instruct\n",
    "# client = InferenceClient(\"https://jc26mwg228mkj8dw.us-east-1.aws.endpoints.huggingface.cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217c70c-bbdd-4d99-b75d-086f0634a489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a75fa73-0610-492e-b853-a824ac7e19dd",
   "metadata": {},
   "source": [
    "## without CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b461383a-4bb8-47bc-b578-2d236a519dc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/gpt2 (Request ID: Root=1-68372ca6-7341dce4745284325cb0e0ee;49451691-d3eb-4265-9460-9fba35685e22)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\requests\\models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/gpt2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m output = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe capital of France is\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2340\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2315\u001b[39m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[32m   2316\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2317\u001b[39m             prompt=prompt,\n\u001b[32m   2318\u001b[39m             details=details,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2338\u001b[39m             watermark=watermark,\n\u001b[32m   2339\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[43mraise_text_generation_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2342\u001b[39m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_common.py:410\u001b[39m, in \u001b[36mraise_text_generation_error\u001b[39m\u001b[34m(http_error)\u001b[39m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttp_error\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2310\u001b[39m, in \u001b[36mInferenceClient.text_generation\u001b[39m\u001b[34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[39m\n\u001b[32m   2308\u001b[39m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[32m   2309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2310\u001b[39m     bytes_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2311\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2312\u001b[39m     match = MODEL_KWARGS_NOT_USED_REGEX.search(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:280\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    277\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\PycharmProjects\\LLM_Agents\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/gpt2 (Request ID: Root=1-68372ca6-7341dce4745284325cb0e0ee;49451691-d3eb-4265-9460-9fba35685e22)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "output = client.text_generation(\n",
    "    \"The capital of France is\",\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e64444-902a-4371-a392-611e9f609bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e338466-21c1-42f8-b901-61b96c5af686",
   "metadata": {},
   "source": [
    "## with CHAT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8743f0d9-aa4f-4af2-beba-6a6a2b1ad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "The capital of France is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "output = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c4a29-6c76-4111-95c1-a07243987df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "217b542e-e2c2-4cd8-9d49-fcb0c7bd7918",
   "metadata": {},
   "source": [
    "## Chat method: RECOMMENDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c758456-8cc4-47c2-8181-38170525fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"The capital of France is\"\n",
    "     }]\n",
    "output = client.chat.completions.create(messages=prompt_messages,\n",
    "                                        stream=False,\n",
    "                                        max_tokens=1024\n",
    "                                        )\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e007ee25-63a7-4339-bbd0-71ca9076191d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78549c56-0d03-4ed4-9f5e-6afbfdb8483d",
   "metadata": {},
   "source": [
    "## SYSTEM PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e1ab9-3e19-4215-b9c2-b0b0abbf973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This system prompt is a bit more complex and actually contains the function description already appended.\n",
    "# Here we suppose that the textual description of the tools has already been appended.\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "get_weather: Get the current weather in a given location\n",
    "\n",
    "The way you use the tools is by specifying a json blob.\n",
    "Specifically, this json should have an `action` key (with the name of the tool to use) and an `action_input` key (with the input to the tool going here).\n",
    "\n",
    "The only values that should be in the \"action\" field are:\n",
    "get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}}\n",
    "example use :\n",
    "\n",
    "{{\n",
    "  \"action\": \"get_weather\",\n",
    "  \"action_input\": {\"location\": \"New York\"}\n",
    "}}\n",
    "\n",
    "\n",
    "ALWAYS use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about one action to take. Only one action at a time in this format:\n",
    "Action:\n",
    "\n",
    "$JSON_BLOB (inside markdown cell)\n",
    "\n",
    "Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
    "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
    "\n",
    "You must always end your output with the following format:\n",
    "\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5eb2f-3514-4a0f-915a-cddfdf489676",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{SYSTEM_PROMPT}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "What's the weather in London ?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "# OR\n",
    "# messages=[\n",
    "#     {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "#     {\"role\": \"user\", \"content\": \"What's the weather in London ?\"},\n",
    "#     ]\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "# tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e2dee-72a4-424d-859c-f80d7e3f4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6049f-c176-4b30-b2d9-7f6350c5d30c",
   "metadata": {},
   "source": [
    "### stop argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f7895-8605-4b26-b2f4-8e758b49db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    stop=[\"Observation:\"] # Let's stop before any actual function is called\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ced62-28a4-4f4c-a560-37daba39a600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a55b72dd-1449-4b34-a09d-afad302ac536",
   "metadata": {},
   "source": [
    "### Dummy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf759ab-e2a9-4fd6-9535-4cd3f9ee1a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location):\n",
    "    return f\"the weather in {location} is sunny with low temperatures. \\n\"\n",
    "\n",
    "get_weather('London')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f63e8-e950-4bc5-b3d0-4d44a03d51f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prompt = prompt + output + get_weather('London')\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7dc93d-66b2-4994-a77a-c81c5bfc56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = client.text_generation(\n",
    "    new_prompt,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66544493-4414-49b1-949f-cc718c2fd7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc959bb5-2030-4696-a694-3e94cf359cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agents",
   "language": "python",
   "name": "llm_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
